**Name**: Feiyang Jiang

**Email**: fejiang@ucsd.edu
<br>

*Section*: B04

*Instructor*: Hao Zhang
<br>

**1. What is the most interesting topic covered in your domain this quarter?**

   The most interesting topic covered in my domain (large language model) this quarter is the concept of model flop utilization in assessing efficiency, particularly as it relates to scaling laws. Scaling laws provide insights into how model performance improves with increased computation, data, and parameters, making FLOP utilization a critical metric for optimizing trade-offs. I have heard about those two concepts when taking the system course, however, it is not until this quarter when I have the chance to implement a scalable training pipeline that I get a sense of what those are like. It comes into play especially under situations where computational resources are limited, which I think is a very interesting topic to learn.

**2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.**

   One potential investigation that I would like to pursue for my Quarter 2 Project is to improve the fine-tuning process of large language models for domain-specific tasks. This could involve investigating techniques such as parameter-efficient fine-tuning (e.g., LoRA or adapter layers) to reduce computational requirements while maintaining or enhancing model performance. I am particularly interested in evaluating how these approaches balance efficiency and adaptability across different domains, especially for applications with limited data. Such an investigation could offer valuable insights into making large language models more accessible and practical in specialized fields.

**3. What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?**

   For my Quarter 1 Project on training pipeline, one potential change I would like to make is to dive deeper into fine tuning the training hyperparameters of the model and the size of the dataset by doing a closer examination on the memory and distribution of processes during parallelization. This might help with improving the training speed of the model, and might help with finding the balance between training speed and model accuracy. 

**4. What other techniques would you be interested in using in your project?**

   One example of techniques I would be interested in using for my project includes experimenting with quantization to reduce memory usage during model training while maintaining performance. Additionally, I aim to explore other parallelization methods, such as leveraging accelerators and frameworks like PyTorch, to optimize resource allocation and improve computational efficiency.
